{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - DL Tutorial 1 \n",
    "\n",
    "Please complete the following notebook and submit your solutions to manuel.milling@informatik.uni-augsburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions from exercise sheet 3 (class methods below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainx shape: (60000, 784)\n",
      "Trainy shape: (60000,)\n",
      "Testx shape:  (10000, 784)\n",
      "Testy shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "#Equations from handout 3 and for are referred to as (3.X) and (4.X)\n",
    "\n",
    "import numpy as np\n",
    "#numpy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "trainx, trainy, testx, testy = np.load('mnist.npy', allow_pickle=True)\n",
    "print(\"Trainx shape: {}\".format(trainx.shape))\n",
    "print(\"Trainy shape: {}\".format(trainy.shape))\n",
    "print(\"Testx shape:  {}\".format(testx.shape))\n",
    "print(\"Testy shape:  {}\".format(testy.shape))\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1 +np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "    #more stable\n",
    "    eps = X.max()\n",
    "    return np.exp(X + eps)/(np.sum(np.exp(X + eps), axis=1).reshape((X.shape[0],1)))\n",
    "\n",
    "def fcc_one_layer(X, W, b, activation):\n",
    "    return activation(np.matmul(X, W) + b)\n",
    "\n",
    "def cross_entropy(pred_logits, y):\n",
    "    num_data_points = pred_logits.shape[0]\n",
    "    correct_logits = pred_logits[np.arange(num_data_points),y]\n",
    "    return np.mean(-np.log(correct_logits))\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    class_predictions = np.argmax(logits, axis=1)\n",
    "    return np.mean(class_predictions == labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Implement the error of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_last_layer(h, y):\n",
    "    \"\"\"\n",
    "    :param h: softmax activations of shape (num_examples, num_classes)\n",
    "    :param y: correct labels of shape num_classes\n",
    "    :return: delta of softmax\n",
    "    \"\"\"\n",
    "    num_data_points = h.shape[0]\n",
    "    # get H^n_mi for (4.31)\n",
    "    correct_logits = h[np.arange(num_data_points), y]\n",
    "    # (4.31) i no equal j\n",
    "    h_i_neq_j = - np.reshape(correct_logits, (correct_logits.shape[0], 1)) * h\n",
    "    # (4.31) i=j\n",
    "    h_i_eq_j = correct_logits*(1- correct_logits)\n",
    "    #replace the i=j terms in i not equal j matrix\n",
    "    h_i_neq_j[np.arange(num_data_points),y] = h_i_eq_j\n",
    "    #(4.30)\n",
    "    h_i_neq_j = h_i_neq_j / np.reshape(correct_logits, (correct_logits.shape[0], 1))\n",
    "    #transpose h --> delta shape\n",
    "    return - np.transpose(h_i_neq_j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_last_layer_easy_approach(h, y):\n",
    "    # create one hot vectors for every row\n",
    "    one_hots = np.zeros((h.shape[0], h.shape[1]))\n",
    "    one_hots[np.arange(h.shape[0]), y] = 1.0\n",
    "    # (4.30) and (4.31) can be reshsaped\n",
    "    return (h - one_hots).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Implement the derivative of the sigmoid function in terms of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_sigmoid(h):\n",
    "    \"\"\"\n",
    "    :param h: output of sigmoid function, i.e. h = \\sigma(i)\n",
    "    \"\"\"\n",
    "    #dh/dx = d\\sigma(x)/dx = \\sigma(x)(1-\\sigma(x)) = h(1-h)\n",
    "    return h * (1 - h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.   Implement the backpropagation as a class method.\n",
    "4.   Implement the the optimisation step as a class method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcc:\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_out):\n",
    "        # Initialisation and Declaration of class variables\n",
    "        self.W_i_h1 = np.random.randn(n_input, n_hidden1)\n",
    "        self.b_h1 = np.random.randn(n_hidden1)\n",
    "        self.W_h1_h2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "        self.b_h2 = np.random.randn(n_hidden2)\n",
    "        self.W_h2_o = np.random.randn(n_hidden2, n_out)\n",
    "        self.b_out = np.random.randn(n_out)\n",
    "        # not necessary, but for better overview\n",
    "        self.X = None\n",
    "        self.h1 = None\n",
    "        self.h2 = None\n",
    "        self.out = None\n",
    "        self.dW_i_h1 = None\n",
    "        self.db_h1 = None\n",
    "        self.dW_h1_h2 = None\n",
    "        self.db_h2 = None\n",
    "        self.dW_h2_o = None\n",
    "        self.db_out = None\n",
    "\n",
    "        #calculation of network parameters\n",
    "        n_trainable_bias = self.b_h1.shape[0] + self.b_h2.shape[0] + self.b_out.shape[0]\n",
    "        n_trainable_weights = self.W_i_h1.shape[0] * self.W_i_h1.shape[1] + self.W_h1_h2.shape[0] * self.W_h1_h2.shape[1] + self.W_h2_o.shape[0] * self.W_h2_o.shape[1]\n",
    "        print(\"Number of parameters: {}\".format(n_trainable_bias + n_trainable_weights))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.X = X\n",
    "        # (3.4)\n",
    "        self.h1 = fcc_one_layer(X, self.W_i_h1, self.b_h1, sigmoid)\n",
    "        # (3.4)\n",
    "        self.h2 = fcc_one_layer(self.h1, self.W_h1_h2, self.b_h2, sigmoid)\n",
    "        # (3.4)\n",
    "        self.out = fcc_one_layer(self.h2, self.W_h2_o, self.b_out, softmax)\n",
    "        return self.out\n",
    "\n",
    "    def backprop(self, y):\n",
    "        self.num_train_ex = y.shape[0]\n",
    "        self.delta_out = delta_last_layer_easy_approach(self.out, y)\n",
    "        # (4.27)\n",
    "        self.dW_h2_o = np.transpose(np.matmul(self.delta_out, self.h2))/self.num_train_ex\n",
    "        # (4.28)\n",
    "        self.db_out = np.mean(self.delta_out, axis=1)\n",
    "        # (4.26)\n",
    "        self.delta_h2 = np.matmul(self.W_h2_o, self.delta_out) * np.transpose(del_sigmoid(self.h2))\n",
    "        # (4.27)\n",
    "        self.dW_h1_h2 = np.transpose(np.matmul(self.delta_h2, self.h1)) / self.num_train_ex\n",
    "        # (4.28)\n",
    "        self.db_h2 = np.mean(self.delta_h2, axis=1)\n",
    "        # (4.26)\n",
    "        self.delta_h1 = np.matmul(self.W_h1_h2, self.delta_h2) * np.transpose(del_sigmoid(self.h1))\n",
    "        # (4.27)\n",
    "        self.dW_i_h1 = np.transpose(np.matmul(self.delta_h1, self.X)) / self.num_train_ex\n",
    "        # (4.28)\n",
    "        self.db_h1 = np.mean(self.delta_h1, axis=1)\n",
    "        # observe magnitude of gradient (see email on this topic)\n",
    "        #grad_magn = np.sum(self.dW_h2_o**2) +np.sum(self.db_out**2)+np.sum(self.dW_h1_h2**2)+np.sum(self.db_h2**2)+np.sum(self.dW_i_h1**2)+np.sum(self.db_out**2)\n",
    "        #print(\"gradient magn: {}\".format(grad_magn))\n",
    "\n",
    "    def gradient_step(self, learning_rate):\n",
    "        self.learning_rate = learning_rate\n",
    "        # (4.2), note the wrong equation in the previous version of the handout, updated now.\n",
    "        self.W_i_h1 -= self.learning_rate * self.dW_i_h1\n",
    "        self.b_h1 -= self.learning_rate * self.db_h1\n",
    "        self.W_h1_h2 -= self.learning_rate * self.dW_h1_h2\n",
    "        self.b_h2 -= self.learning_rate * self.db_h2\n",
    "        self.W_h2_o -= self.learning_rate * self.dW_h2_o\n",
    "        self.b_out -= self.learning_rate * self.db_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   Implement the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 478410\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "neural_net = fcc(784, 400, 400, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t0\n",
      "Test Loss:\t\t19.479279548133853\n",
      "Test Accurcy:\t\t0.1353\n",
      "Train Loss:\t\t19.653523403657122\n",
      "Train Accuracy:\t\t0.13226666666666667\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t100\n",
      "Test Loss:\t\t2.282769668031916\n",
      "Test Accurcy:\t\t0.5941\n",
      "Train Loss:\t\t2.4319569058890083\n",
      "Train Accuracy:\t\t0.57965\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t200\n",
      "Test Loss:\t\t1.573850466163723\n",
      "Test Accurcy:\t\t0.7007\n",
      "Train Loss:\t\t1.6520382064719452\n",
      "Train Accuracy:\t\t0.6910666666666667\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t300\n",
      "Test Loss:\t\t1.296911010188398\n",
      "Test Accurcy:\t\t0.7445\n",
      "Train Loss:\t\t1.3371711957554957\n",
      "Train Accuracy:\t\t0.7404666666666667\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t400\n",
      "Test Loss:\t\t1.142345153274019\n",
      "Test Accurcy:\t\t0.7731\n",
      "Train Loss:\t\t1.1521656169086645\n",
      "Train Accuracy:\t\t0.7698666666666667\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t500\n",
      "Test Loss:\t\t1.0402612586580389\n",
      "Test Accurcy:\t\t0.7876\n",
      "Train Loss:\t\t1.0261024142379838\n",
      "Train Accuracy:\t\t0.7897\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t600\n",
      "Test Loss:\t\t0.9656370013641344\n",
      "Test Accurcy:\t\t0.8009\n",
      "Train Loss:\t\t0.9324061314836917\n",
      "Train Accuracy:\t\t0.8044166666666667\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t700\n",
      "Test Loss:\t\t0.9074886004541431\n",
      "Test Accurcy:\t\t0.8102\n",
      "Train Loss:\t\t0.8590248278384216\n",
      "Train Accuracy:\t\t0.8161333333333334\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t800\n",
      "Test Loss:\t\t0.8604017885240762\n",
      "Test Accurcy:\t\t0.8191\n",
      "Train Loss:\t\t0.7996394461404814\n",
      "Train Accuracy:\t\t0.8266333333333333\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t900\n",
      "Test Loss:\t\t0.8212557955885055\n",
      "Test Accurcy:\t\t0.8263\n",
      "Train Loss:\t\t0.7502771993917843\n",
      "Train Accuracy:\t\t0.8350833333333333\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Iteration:\t\t999\n",
      "Test Loss:\t\t0.7880334811680907\n",
      "Test Accurcy:\t\t0.8321\n",
      "Train Loss:\t\t0.7083240971614022\n",
      "Train Accuracy:\t\t0.8421666666666666\n"
     ]
    }
   ],
   "source": [
    "#1000 trainingssteps\n",
    "num_iterations = 1000\n",
    "for i in range(num_iterations):\n",
    "    # evaluate after each 100 steps\n",
    "    if i % 100 == 0:\n",
    "        print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Iteration:\\t\\t{}\".format(i))\n",
    "        logits = neural_net.forward_propagation(testx)\n",
    "        print(\"Test Loss:\\t\\t{}\".format(cross_entropy(logits, testy)))\n",
    "        print(\"Test Accurcy:\\t\\t{}\".format(accuracy(logits, testy)))\n",
    "    logits = neural_net.forward_propagation(trainx)\n",
    "    if i%100 == 0:\n",
    "        print(\"Train Loss:\\t\\t{}\".format(cross_entropy(logits, trainy)))\n",
    "        print(\"Train Accuracy:\\t\\t{}\".format(accuracy(logits, trainy)))\n",
    "        print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "    neural_net.backprop(trainy)\n",
    "    neural_net.gradient_step(learning_rate)\n",
    "print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "print(\"Iteration:\\t\\t{}\".format(i))\n",
    "logits = neural_net.forward_propagation(testx)\n",
    "print(\"Test Loss:\\t\\t{}\".format(cross_entropy(logits, testy)))\n",
    "print(\"Test Accurcy:\\t\\t{}\".format(accuracy(logits, testy)))\n",
    "logits = neural_net.forward_propagation(trainx)\n",
    "print(\"Train Loss:\\t\\t{}\".format(cross_entropy(logits, trainy)))\n",
    "print(\"Train Accuracy:\\t\\t{}\".format(accuracy(logits, trainy)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t0\n",
      "Test Loss:\t\t20.258685506517264\n",
      "Test Accurcy:\t\t0.0926\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t19.821905152851123\n",
      "Train Accuracy:\t\t0.109375\n",
      "Train Loss:\t\t1.8949693999235606\n",
      "Train Accuracy:\t\t0.5625\n",
      "Train Loss:\t\t2.0527701690070286\n",
      "Train Accuracy:\t\t0.671875\n",
      "Train Loss:\t\t1.6121803157171737\n",
      "Train Accuracy:\t\t0.65625\n",
      "Train Loss:\t\t2.077289663492683\n",
      "Train Accuracy:\t\t0.71875\n",
      "Train Loss:\t\t1.453578726376255\n",
      "Train Accuracy:\t\t0.71875\n",
      "Train Loss:\t\t1.3055405866063823\n",
      "Train Accuracy:\t\t0.640625\n",
      "Train Loss:\t\t1.0969084533218267\n",
      "Train Accuracy:\t\t0.828125\n",
      "Train Loss:\t\t0.7339847306076936\n",
      "Train Accuracy:\t\t0.796875\n",
      "Train Loss:\t\t0.717445025227063\n",
      "Train Accuracy:\t\t0.828125\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t1\n",
      "Test Loss:\t\t0.829246421718839\n",
      "Test Accurcy:\t\t0.8072\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.7272308116995612\n",
      "Train Accuracy:\t\t0.859375\n",
      "Train Loss:\t\t0.4546252790733857\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.41340715768929903\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.5671620821361496\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.607704523731818\n",
      "Train Accuracy:\t\t0.859375\n",
      "Train Loss:\t\t0.9237634688270586\n",
      "Train Accuracy:\t\t0.828125\n",
      "Train Loss:\t\t0.6420787375359898\n",
      "Train Accuracy:\t\t0.828125\n",
      "Train Loss:\t\t0.6920089752589389\n",
      "Train Accuracy:\t\t0.796875\n",
      "Train Loss:\t\t0.5326557310728961\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.5589444050035213\n",
      "Train Accuracy:\t\t0.859375\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t2\n",
      "Test Loss:\t\t0.6006060028525041\n",
      "Test Accurcy:\t\t0.8547\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.21403026054300162\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.5835133616491388\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.21846635549655236\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.33095053937109997\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.4736054595929376\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.3181131195462824\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t1.0971615437079558\n",
      "Train Accuracy:\t\t0.796875\n",
      "Train Loss:\t\t0.8873908002508526\n",
      "Train Accuracy:\t\t0.796875\n",
      "Train Loss:\t\t0.44946825910555793\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.5041590311200393\n",
      "Train Accuracy:\t\t0.875\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t3\n",
      "Test Loss:\t\t0.541864884565748\n",
      "Test Accurcy:\t\t0.8657\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.41339002318289053\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.8176515338162411\n",
      "Train Accuracy:\t\t0.859375\n",
      "Train Loss:\t\t0.40132781782295684\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.33430511847630184\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.7221287013493025\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.2839316870575459\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.6324136504595422\n",
      "Train Accuracy:\t\t0.84375\n",
      "Train Loss:\t\t0.3031215537530683\n",
      "Train Accuracy:\t\t0.859375\n",
      "Train Loss:\t\t0.28820117605738405\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.29782516246743795\n",
      "Train Accuracy:\t\t0.921875\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t4\n",
      "Test Loss:\t\t0.4795243111168984\n",
      "Test Accurcy:\t\t0.8786\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.3266281853191817\n",
      "Train Accuracy:\t\t0.859375\n",
      "Train Loss:\t\t0.5703808865286903\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.6084037879009787\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.33905053251089184\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.33384748068021036\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.09315117497248468\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.41554701810602396\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.42168287669369964\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.09973477851571244\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.16285784441257567\n",
      "Train Accuracy:\t\t0.9375\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t5\n",
      "Test Loss:\t\t0.45181655266448134\n",
      "Test Accurcy:\t\t0.8853\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.3338319930056494\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.21061301890554218\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.43509240214523875\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.3819723565265545\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.21246479319330602\n",
      "Train Accuracy:\t\t0.953125\n",
      "Train Loss:\t\t0.2376686571606047\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.25151420713191086\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.20354643338351447\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.18678610779087723\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.22387500423116447\n",
      "Train Accuracy:\t\t0.90625\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t6\n",
      "Test Loss:\t\t0.41825468432712576\n",
      "Test Accurcy:\t\t0.8944\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.21960927544288905\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.5886084068574503\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.31915889007644227\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.2640351986849304\n",
      "Train Accuracy:\t\t0.953125\n",
      "Train Loss:\t\t0.18306732987664714\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.09235001623706189\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.05787261271787962\n",
      "Train Accuracy:\t\t0.984375\n",
      "Train Loss:\t\t0.2934668839123454\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.19232986736356816\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.11129772639575122\n",
      "Train Accuracy:\t\t0.96875\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t7\n",
      "Test Loss:\t\t0.4036277831130596\n",
      "Test Accurcy:\t\t0.8977\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.23094993036100708\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.36184873834933773\n",
      "Train Accuracy:\t\t0.875\n",
      "Train Loss:\t\t0.07741029852373713\n",
      "Train Accuracy:\t\t0.984375\n",
      "Train Loss:\t\t0.0848325111041058\n",
      "Train Accuracy:\t\t0.984375\n",
      "Train Loss:\t\t0.1536494925764389\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.2310402896812545\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.15965924835139556\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.19322004947954186\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.21794990631606664\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.332240658462243\n",
      "Train Accuracy:\t\t0.9375\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t8\n",
      "Test Loss:\t\t0.3906703446680485\n",
      "Test Accurcy:\t\t0.8984\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.2867269970871544\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.3814632468990543\n",
      "Train Accuracy:\t\t0.890625\n",
      "Train Loss:\t\t0.1436071386204857\n",
      "Train Accuracy:\t\t0.953125\n",
      "Train Loss:\t\t0.23629611451358562\n",
      "Train Accuracy:\t\t0.953125\n",
      "Train Loss:\t\t0.10435398836665546\n",
      "Train Accuracy:\t\t0.984375\n",
      "Train Loss:\t\t0.5781831720274652\n",
      "Train Accuracy:\t\t0.796875\n",
      "Train Loss:\t\t0.08227742887304698\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.13813027119776133\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.08399328565595478\n",
      "Train Accuracy:\t\t0.984375\n",
      "Train Loss:\t\t0.5653321104290344\n",
      "Train Accuracy:\t\t0.84375\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Epoch:\t\t9\n",
      "Test Loss:\t\t0.3833463037479325\n",
      "Test Accurcy:\t\t0.9027\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Train Loss:\t\t0.10712263831657187\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.22736066565642798\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.07907505963907441\n",
      "Train Accuracy:\t\t0.984375\n",
      "Train Loss:\t\t0.30858628790199494\n",
      "Train Accuracy:\t\t0.90625\n",
      "Train Loss:\t\t0.3362755617727109\n",
      "Train Accuracy:\t\t0.921875\n",
      "Train Loss:\t\t0.05712151144995843\n",
      "Train Accuracy:\t\t1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss:\t\t0.25191368081346033\n",
      "Train Accuracy:\t\t0.953125\n",
      "Train Loss:\t\t0.13200809246434136\n",
      "Train Accuracy:\t\t0.9375\n",
      "Train Loss:\t\t0.25899725908977705\n",
      "Train Accuracy:\t\t0.96875\n",
      "Train Loss:\t\t0.2792175035318819\n",
      "Train Accuracy:\t\t0.921875\n",
      "-----------------------------------------------------------------------------------------------------\n",
      "Final:\t\t\n",
      "Test Loss:\t\t0.3752001490038708\n",
      "Train Accuracy:\t\t0.9058\n"
     ]
    }
   ],
   "source": [
    "#rerun initialisation of neural_network before executing\n",
    "\n",
    "batch_size = 64\n",
    "permutation = np.arange(trainx.shape[0])\n",
    "epochs = 10\n",
    "for i in range(epochs):\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Epoch:\\t\\t{}\".format(i))\n",
    "    logits = neural_net.forward_propagation(testx)\n",
    "    print(\"Test Loss:\\t\\t{}\".format(cross_entropy(logits, testy)))\n",
    "    print(\"Test Accurcy:\\t\\t{}\".format(accuracy(logits, testy)))\n",
    "    print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "    #create new permuatation of trainings examples\n",
    "    np.random.shuffle(permutation)\n",
    "    #loop over epoch (= one permutation of all data)\n",
    "    for j in range(int(trainx.shape[0]/batch_size)):\n",
    "        #take one minibatch\n",
    "        batch = permutation[j*batch_size:(j+1) * batch_size]\n",
    "        trainx_batch = trainx[batch]\n",
    "        trainy_batch = trainy[batch]\n",
    "        logits = neural_net.forward_propagation(trainx_batch)\n",
    "        # print every 100 training steps\n",
    "        if j%100 == 0:\n",
    "            print(\"Train Loss:\\t\\t{}\".format(cross_entropy(logits, trainy_batch)))\n",
    "            print(\"Train Accuracy:\\t\\t{}\".format(accuracy(logits, trainy_batch)))\n",
    "        neural_net.backprop(trainy_batch)\n",
    "        neural_net.gradient_step(learning_rate)\n",
    "logits = neural_net.forward_propagation(testx)\n",
    "print(\"-----------------------------------------------------------------------------------------------------\")\n",
    "print(\"Final:\\t\\t\")\n",
    "print(\"Test Loss:\\t\\t{}\".format(cross_entropy(logits, testy)))\n",
    "print(\"Train Accuracy:\\t\\t{}\".format(accuracy(logits, testy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
