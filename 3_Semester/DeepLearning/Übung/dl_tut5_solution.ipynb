{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0mZhmra3mcz6"
   },
   "source": [
    "## Exercise - DL Tutorial 5\n",
    "\n",
    "Please complete the following notebook and submit it by the 25th Nov 23:59 to manuel.milling@informatik.uni-augsburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HrSPQusomcz-"
   },
   "source": [
    "## student name: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nStp1HcKmc0T"
   },
   "outputs": [],
   "source": [
    "# Equation numbers refer to handout 5\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(X):\n",
    "    return 1/(1 +np.exp(-X))\n",
    "\n",
    "def del_sigmoid(h):\n",
    "    return h * (1 - h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSBUWqkemc0Y"
   },
   "source": [
    "Implement a method that creates binary addition data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CcQvj8Xmmc0Z"
   },
   "outputs": [],
   "source": [
    "def generate_data(num_examples, max_len):\n",
    "    # generate num_examples * 2 ints\n",
    "    rand_numbers = np.random.randint(0, 2**(max_len-1)-1, size=(num_examples * 2), dtype=np.uint8)\n",
    "    rand_numbers_bits = np.unpackbits(rand_numbers)\n",
    "    rand_numbers = rand_numbers.reshape(num_examples, 2)\n",
    "    rand_numbers_bits = rand_numbers_bits.reshape(num_examples, 2, max_len)\n",
    "    rand_results = np.sum(rand_numbers, axis=1, dtype=np.uint8)\n",
    "    # add 3rd dimension to tensor\n",
    "    rand_results_bits = np.unpackbits(rand_results).reshape(num_examples, max_len, 1)\n",
    "    # data should be of form (num_examples, sequence_length, num_features)\n",
    "    rand_numbers_bits = np.transpose(rand_numbers_bits, axes=[0,2,1])\n",
    "\n",
    "    return rand_numbers_bits, rand_results_bits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cu4-nHGYmc0d"
   },
   "source": [
    "Implement the mean squared error as a loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Swdz9olbmc0e"
   },
   "outputs": [],
   "source": [
    "def mean_square_error(pred, y):\n",
    "    return np.mean((pred-y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8-6Wz94Amc0j"
   },
   "source": [
    "Iimplement the accuracy of the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4KFYwvuFmc0k"
   },
   "outputs": [],
   "source": [
    "def accuracy(pred, y):\n",
    "    rounded = np.rint(pred)\n",
    "    return np.mean(rounded==y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oxdJMootmc0o"
   },
   "source": [
    "Implement the RNN class, implement the forward propagation, implement the BPTT and implement the gradient step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jbV0BbyFmc0q"
   },
   "outputs": [],
   "source": [
    "class one_layer_rnn:\n",
    "    def __init__(self, n_input, n_hidden, n_out):\n",
    "        #initialisation of weights, no bias\n",
    "        self.W_1 = np.random.randn(n_input, n_hidden)\n",
    "        self.U = np.random.randn(n_hidden, n_hidden)\n",
    "        self.W_2 = np.random.randn(n_hidden, n_out)\n",
    "        self.X = None\n",
    "        self.H = None\n",
    "        self.out = None\n",
    "        self.dW1 = None\n",
    "        self.db_h1 = None\n",
    "        self.dU = None\n",
    "        self.dW2 = None\n",
    "        self.db_out = None\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.X = X\n",
    "        # \"dot\" multiplication of X and W_1 is performed over the last dimension of X (the features for one sequence\n",
    "        # and one training example) and W_1. Result: H without any horizontal information flow.\n",
    "        # (2)\n",
    "        self.H = np.dot(self.X, self.W_1)\n",
    "        prev = np.zeros(self.H[:, 0, :].shape)\n",
    "        # loop over sequence\n",
    "        # numbers have to be added from right to left\n",
    "        for i in range(self.X.shape[1]-1, -1, -1):\n",
    "            # matrix multiplication of ith element of sequence\n",
    "            # (2)\n",
    "            self.H[:, i, :] = sigmoid(self.H[:, i, :] + np.dot(prev, self.U))\n",
    "            prev = self.H[:, i, :]\n",
    "        # (3)\n",
    "        self.out = sigmoid(np.dot(self.H, self.W_2))\n",
    "        return self.out\n",
    "\n",
    "    def backprop_through_time(self, Y):\n",
    "        # derivative of mean-square error\n",
    "        # dimension of delta for matrix multiplication: num_sequence, num_feature (1), num_examples\n",
    "        # (6)\n",
    "        self.d_out = np.transpose(2*(self.out - Y) * del_sigmoid(self.out), [1,2,0])\n",
    "        #print(self.d_out.shape)\n",
    "        self.dW2 = np.zeros(self.W_2.shape)\n",
    "        # backprop: left to right.\n",
    "        for i in range(self.X.shape[1]):\n",
    "            # sum up contribution of all sequence results to dW2.\n",
    "            # basically sum over (5), like in (14)\n",
    "            self.dW2 += np.transpose(np.matmul(self.d_out[i,:,:], self.H[:,i,:]))\n",
    "        # (13): W^n \\delta^{n,\\tau} part, vertical backprop\n",
    "        # sum of dot multiplication is second-to last index of d_out\n",
    "        # --> transpose to get num_features back to second index\n",
    "        self.d_hidden = np.transpose(np.dot(self.W_2, self.d_out), [1,0,2])\n",
    "        #print(self.d_hidden.shape)\n",
    "        prev = np.zeros(self.d_hidden[0, :, :].shape)\n",
    "        #backprop: left to right\n",
    "        for i in range(self.X.shape[1]):\n",
    "            # (13): U^{n-1} \\delta^{n-1,\\tau + 1} part, horizontal backprop\n",
    "            self.d_hidden[i,:,:] += np.matmul(self.U, prev)\n",
    "            self.d_hidden[i, :, :] *= np.transpose(del_sigmoid(self.H[:,i,:]))\n",
    "            prev = self.d_hidden[i, :, :]\n",
    "        # average gradient for every sequence element\n",
    "        self.dW2 /= self.X.shape[0]\n",
    "        self.dW1 = np.zeros(self.W_1.shape)\n",
    "        for i in range(self.X.shape[1]):\n",
    "            # (13) only vertical backprop necessary\n",
    "            self.dW1 += np.transpose(np.matmul(self.d_hidden[i,:,:], self.X[:,i,:]))\n",
    "        self.dW1/= self.X.shape[0]\n",
    "        self.dU =  np.zeros(self.U.shape)\n",
    "        for i in range(self.X.shape[1]-1):\n",
    "            self.dU += np.transpose(np.matmul(self.d_hidden[i, :, :], self.H[:, i+1, :]))\n",
    "        self.dU /= self.X.shape[0]\n",
    "\n",
    "    def gradient_step(self, learning_rate):\n",
    "        self.U -= learning_rate*self.dU\n",
    "        self.W_2 -= learning_rate * self.dW2\n",
    "        self.W_1 -= learning_rate * self.dW1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qPgxqLskmc0u"
   },
   "source": [
    "Implement the learning routine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NRozmS-Dmc0w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: \t0.5365197757564597\n",
      "Test acc: \t0.425\n",
      "Train loss: \t0.4856609082054297\n",
      "Train acc: \t0.48625\n",
      "Test loss: \t0.25153028086883544\n",
      "Test acc: \t0.5\n",
      "Train loss: \t0.2481795358491336\n",
      "Train acc: \t0.555\n",
      "Test loss: \t0.2455683905001258\n",
      "Test acc: \t0.475\n",
      "Train loss: \t0.2443045985320066\n",
      "Train acc: \t0.52375\n",
      "Test loss: \t0.24100159227377316\n",
      "Test acc: \t0.55\n",
      "Train loss: \t0.24190537041769664\n",
      "Train acc: \t0.5575\n",
      "Test loss: \t0.23692596905867064\n",
      "Test acc: \t0.5625\n",
      "Train loss: \t0.23915349581434533\n",
      "Train acc: \t0.57375\n",
      "Test loss: \t0.2323586025516562\n",
      "Test acc: \t0.6\n",
      "Train loss: \t0.23524240649455563\n",
      "Train acc: \t0.57375\n",
      "Test loss: \t0.22666308229285786\n",
      "Test acc: \t0.6\n",
      "Train loss: \t0.2293833258167404\n",
      "Train acc: \t0.59125\n",
      "Test loss: \t0.21952703145298597\n",
      "Test acc: \t0.6\n",
      "Train loss: \t0.2211532472825828\n",
      "Train acc: \t0.60625\n",
      "Test loss: \t0.21082664631622708\n",
      "Test acc: \t0.6625\n",
      "Train loss: \t0.2105662609641598\n",
      "Train acc: \t0.64125\n",
      "Test loss: \t0.19978939666508397\n",
      "Test acc: \t0.725\n",
      "Train loss: \t0.19728047678793792\n",
      "Train acc: \t0.71375\n",
      "Test loss: \t0.18421501848956834\n",
      "Test acc: \t0.75\n",
      "Train loss: \t0.18050648500512267\n",
      "Train acc: \t0.775\n",
      "Test loss: \t0.162216799167218\n",
      "Test acc: \t0.775\n",
      "Train loss: \t0.15982103139759585\n",
      "Train acc: \t0.83625\n",
      "Test loss: \t0.13656765530622889\n",
      "Test acc: \t0.8375\n",
      "Train loss: \t0.13667576629601857\n",
      "Train acc: \t0.86875\n",
      "Test loss: \t0.11175745154310199\n",
      "Test acc: \t0.95\n",
      "Train loss: \t0.11352084775104755\n",
      "Train acc: \t0.92625\n",
      "Test loss: \t0.08939666503406743\n",
      "Test acc: \t0.9625\n",
      "Train loss: \t0.09176345909991727\n",
      "Train acc: \t0.95875\n",
      "Test loss: \t0.06927975117187826\n",
      "Test acc: \t0.975\n",
      "Train loss: \t0.07191436417612207\n",
      "Train acc: \t0.985\n",
      "Test loss: \t0.05125229701580747\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.05423395031380389\n",
      "Train acc: \t0.9925\n",
      "Test loss: \t0.036308123968945796\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.03917674547479968\n",
      "Train acc: \t0.99625\n",
      "Test loss: \t0.02562532237468939\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.02781583283119425\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.018765288703792217\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.020422371060917648\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.01444465081627599\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.01572246196003253\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.011618162810805376\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.012629464229314918\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.009668234375068446\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.01049211913683969\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.008254581267221762\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.00894393162416114\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.007186658604857567\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.007776606626018891\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.006352883165191119\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.006867159574555848\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.005684542197095622\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.006139597111132771\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.005137282406172449\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.005544846775367845\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.004681266993557287\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.0050499404963698395\n",
      "Train acc: \t1.0\n",
      "Test loss: \t0.004295702441767664\n",
      "Test acc: \t1.0\n",
      "Train loss: \t0.004631952178283491\n",
      "Train acc: \t1.0\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "train_iters = 3000\n",
    "print_iters = 100\n",
    "\n",
    "trainX, trainY = generate_data(100, 8)\n",
    "testX, testY = generate_data(10, 8)\n",
    "net = one_layer_rnn(2, 16, 1)\n",
    "for i in range(train_iters):\n",
    "    if i% print_iters == 0:\n",
    "        result = net.forward_propagation(testX)\n",
    "        print(\"Test loss: \\t{}\".format(mean_square_error(result, testY)))\n",
    "        print(\"Test acc: \\t{}\".format(accuracy(result, testY)))\n",
    "        result = net.forward_propagation(trainX)\n",
    "        print(\"Train loss: \\t{}\".format(mean_square_error(result, trainY)))\n",
    "        print(\"Train acc: \\t{}\".format(accuracy(result, trainY)))\n",
    "    result = net.forward_propagation(trainX)\n",
    "    net.backprop_through_time(trainY)\n",
    "    net.gradient_step(learning_rate)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DL-T1_excercise.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
