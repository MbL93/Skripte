{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - DL Tutorial 1 \n",
    "\n",
    "Please complete the following notebook and submit your solutions to manuel.milling@informatik.uni-augsburg.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## student name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solutions from exercise sheet 3 (class methods below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainx shape: (60000, 784)\n",
      "Trainy shape: (60000,)\n",
      "Testx shape:  (10000, 784)\n",
      "Testy shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#numpy random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "trainx, trainy, testx, testy = np.load('mnist.npy', allow_pickle=True)\n",
    "print(\"Trainx shape: {}\".format(trainx.shape))\n",
    "print(\"Trainy shape: {}\".format(trainy.shape))\n",
    "print(\"Testx shape:  {}\".format(testx.shape))\n",
    "print(\"Testy shape:  {}\".format(testy.shape))\n",
    "\n",
    "def sigmoid(X):\n",
    "    return 1/(1 +np.exp(-X))\n",
    "\n",
    "def softmax(X):\n",
    "    #more stable\n",
    "    eps = X.max()\n",
    "    return np.exp(X + eps)/(np.sum(np.exp(X + eps), axis=1).reshape((X.shape[0],1)))\n",
    "\n",
    "def fcc_one_layer(X, W, b, activation):\n",
    "    return activation(np.matmul(X, W) + b)\n",
    "\n",
    "def cross_entropy(pred_logits, y):\n",
    "    num_data_points = pred_logits.shape[0]\n",
    "    correct_logits = pred_logits[np.arange(num_data_points),y]\n",
    "    return np.mean(-np.log(correct_logits))\n",
    "\n",
    "def accuracy(logits, labels):\n",
    "    class_predictions = np.argmax(logits, axis=1)\n",
    "    return np.mean(class_predictions == labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   Implement the error of the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_last_layer(h, y):\n",
    "    \"\"\"\n",
    "    :param h: softmax activations of shape (num_examples, num_classes)\n",
    "    :param y: correct labels of shape (num_examples,)\n",
    "    :return: delta of of last layer, i.e. derivative of cross entropy times derivative of softmax\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.   Implement the derivative of the sigmoid function in terms of the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_sigmoid(h):\n",
    "    \"\"\"\n",
    "    :param h: output of the sigmoid function shape (num_examples, num_units)\n",
    "    :return: element-wise derivative of the sigmoid function\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.   Implement the backpropagation as a class method.\n",
    "4.   Implement the the optimisation step as a class method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fcc:\n",
    "    def __init__(self, n_input, n_hidden1, n_hidden2, n_out):\n",
    "        #parameters\n",
    "        self.W_i_h1 = np.random.randn(n_input, n_hidden1)\n",
    "        self.b_h1 = np.random.randn(n_hidden1)\n",
    "        self.W_h1_h2 = np.random.randn(n_hidden1, n_hidden2)\n",
    "        self.b_h2 = np.random.randn(n_hidden2)\n",
    "        self.W_h2_o = np.random.randn(n_hidden2, n_out)\n",
    "        self.b_out = np.random.randn(n_out)\n",
    "        #neuron activations and input H^n\n",
    "        self.X = None\n",
    "        self.h1 = None\n",
    "        self.h2 = None\n",
    "        self.out = None\n",
    "        #components of the gradient\n",
    "        self.dW_i_h1 = None\n",
    "        self.db_h1 = None\n",
    "        self.dW_h1_h2 = None\n",
    "        self.db_h2 = None\n",
    "        self.dW_h2_o = None\n",
    "        self.db_out = None\n",
    "\n",
    "        n_trainable_bias = self.b_h1.shape[0] + self.b_h2.shape[0] + self.b_out.shape[0]\n",
    "        n_trainable_weights = self.W_i_h1.shape[0] * self.W_i_h1.shape[1] + self.W_h1_h2.shape[0] * self.W_h1_h2.shape[1] + self.W_h2_o.shape[0] * self.W_h2_o.shape[1]\n",
    "        print(\"Number of parameters: {}\".format(n_trainable_bias + n_trainable_weights))\n",
    "\n",
    "    def forward_propagation(self, X):\n",
    "        self.X = X\n",
    "        self.h1 = fcc_one_layer(X, self.W_i_h1, self.b_h1, sigmoid)\n",
    "        self.h2 = fcc_one_layer(self.h1, self.W_h1_h2, self.b_h2, sigmoid)\n",
    "        self.out = fcc_one_layer(self.h2, self.W_h2_o, self.b_out, softmax)\n",
    "        return self.out\n",
    "\n",
    "    def backprop(self, y):\n",
    "        \"\"\"\n",
    "        :param y: labels, i.e. numbers of correct classes of shape (num_examples,)\n",
    "        \"\"\"\n",
    "        pass\n",
    "    def gradient_step(self, learning_rate):\n",
    "        \"\"\"\n",
    "        :param learning_rate: learning_rate for training\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.   Implement the training routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 478410\n",
      "Train Loss:\t20.01954426412725\n",
      "Train Accuracy:\t0.11021666666666667\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "num_iterations = 1000\n",
    "\n",
    "neural_net = fcc(784, 400, 400, 10)\n",
    "logits = neural_net.forward_propagation(trainx)\n",
    "print(\"Train Loss:\\t{}\".format(cross_entropy(logits, trainy)))\n",
    "print(\"Train Accuracy:\\t{}\".format(accuracy(logits, trainy)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9.   Which are the parameters we can tune to improve the performance of our network? How many trainable parameters (scalars) does our network have in total?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tunable parameters are all weights and biases, in total 478410, see calculation above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10.  Why did we implement two different evaluation metrics of our system (cross-entropy and accuracy)? What are the main differences between the two and why can’t/shouldn’t we use them interchangeably?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A crucial property of the loss function is that it has to be differentiable at (almost) all points. The accuracy, predicting the class with the highest probability is almost never differentiable, as infinitesimal changes to the network do not change the overall prediction. The cross-entropy however is differentiable in almost every point, so small changes in the network impact the loss function of the network.\n",
    "The accuracy however is the more intuitive and praxis-oriented evaluation metric of a classification problem, as it forces the network to make one prediction. For most applied system it makes more sense to target 100% accuracy, rather than a low probability distribution in false classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
